{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune DistilBERT for binary classification on the SMS Spam Collection\n",
    "__________________________\n",
    "\n",
    "***# This a lot to not say much ^^ We need a real life example specific to DistilBERT doing binary classification on the SMS Spam collection. I also need a clear explanation of why we're using BastionLab because by the end of this tutorial, I really have no idea of the benefit ^^***\n",
    "\n",
    "This can be useful, for instance, when one wants to leverage large pre-trained models on a smaller private dataset, for instance, medical or financial records, and ensure data privacy regarding users' data.\n",
    "\n",
    "BastionLabTorch is intended for scenarios where we have a data owner, for instance, a hospital, wanting to have third parties train models on their data, e.g. a startup, potentially on untrusted infrastructures, such as in the Cloud.\n",
    "\n",
    "The strength of BastionLabTorch is that the data owner can have a high level of protection on data shared to a remote enclave hosted in the Cloud, and operated by the startup, thanks to memory isolation and encryption, and remote attestation from the use of secure enclaves. \n",
    "\n",
    "***# end comment***\n",
    "\n",
    "In this notebook, we will illustrate how BastionLab works. We will use the publicly available dataset [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) to finetune a DistilBERT model on a classification task, to predict whether an email is spam or not.\n",
    "\n",
    "In this guide, we will cover two phases:\n",
    "- The offline phase, in which the data owner prepares the dataset and the data scientist prepares the model.\n",
    "- The online phase, in which dataset and model are uploaded to the secure enclave. In the enclave, the uploaded model will be trained on the dataset. The data scientist can pull the weights once the training is over.\n",
    "\n",
    "We largely followed [this tutorial](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894) to prepare the data and pre-train the model we'll use in this example.\n",
    "\n",
    "## Pre-requisites\n",
    "__________________________\n",
    "\n",
    "We need to have installed: \n",
    "- [BastionLab](https://bastionlab.readthedocs.io/en/latest/docs/getting-started/installation/)\n",
    "- Hugging Face's [Transformers library](https://huggingface.co/docs/transformers/installation)\n",
    "- [Pandas](LINK) ***# PANDAS NEEDS TO BE REPLACED BY POLARS***\n",
    "- [IPython kernel](https://ipython.readthedocs.io/en/stable/install/kernel_install.html) for Jupyter\n",
    "- [Jupyter Widgets](https://ipywidgets.readthedocs.io/en/7.x/user_install.html) to enable notebooks extensions ***# Verify if this is necessary***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bastionlab\n",
    "!pip install transformers pandas ipykernel ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline phase - Model and dataset preparation\n",
    "__________________________________\n",
    "\n",
    "In this section, data owner and data scientist will prepare their data and model so that they are ready-to-use for the training in BastionLab.\n",
    "\n",
    "### Data owner's side: preparing the dataset\n",
    "\n",
    "In this example, our data owner wants a third party data scientist to train an AI model to detect spam from emails.\n",
    "\n",
    "Of course, in a real-world scenario, the data owner already posesses the data, but here, we will need to download one! We'll get the SPAM collection dataset and unzip it by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 17:02:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip.2’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   251KB/s    in 0.8s    \n",
      "\n",
      "2022-12-12 17:02:56 (251 KB/s) - ‘smsspamcollection.zip.2’ saved [203415/203415]\n",
      "\n",
      "Archive:  smsspamcollection.zip\n",
      "replace SMSSpamCollection? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload the dataset to the BastionLab server, the data owner will need to prepare their dataset and make it available in a PyTorch `DataSet` object.\n",
    "\n",
    "***# here we need a line explaining what is happening in the next block of code if what I wrote is wrong. Also, we need some comments in the code running the user through what is going on.***\n",
    "\n",
    "***WE NEED TO CHANGE ALL PANDA TO POLARS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                    Ok lar... Joking wif u oni...\\n\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"\\t\")\n",
    "        labels.append(1 if split[0] == \"spam\" else 0)\n",
    "        texts.append(split[1])\n",
    "df = pd.DataFrame({\"label\": labels, \"text\": texts})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data owner also needs to preprocess the data. We'll use a `DistilBertTokenizer` to obtain tensors ready to be fed to the model:\n",
    "\n",
    "***# ADD COMMENTS IN CODE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "for sample in df.text.values:\n",
    "    encoding_dict = tokenizer.encode_plus(\n",
    "        sample,\n",
    "        add_special_tokens=True,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_id.append(encoding_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "token_id = torch.cat(token_id, dim=0).to(dtype=torch.int64)\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to(dtype=torch.int64)\n",
    "labels = torch.tensor(df.label.values, dtype=torch.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the training process faster in this demonstration, we'll only take a subset of the dataset, but you can choose to take the whole dataset if you want.\n",
    "\n",
    "***# COMMENTS IN THE CODE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_ratio = 0.2\n",
    "limit = 64\n",
    "nb_samples = len(token_id)\n",
    "\n",
    "idx = np.arange(nb_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[int(test_ratio * nb_samples) :][:limit]\n",
    "test_idx = idx[: int(test_ratio * nb_samples)][:limit]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our training and validation `TensorDataset` objects. We'll use them to wrap our `Tensor` objects into a PyTorch `DataSet`.\n",
    "\n",
    "***# LITTLE COMMENTS IN THE CODE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab.torch.utils import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(\n",
    "    [token_id[train_idx], attention_masks[train_idx]], labels[train_idx]\n",
    ")\n",
    "\n",
    "validation_set = TensorDataset(\n",
    "    [token_id[test_idx], attention_masks[test_idx]], labels[test_idx]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: preparing the model\n",
    "\n",
    "On his side, the data scientist must prepare the DistilBERT language model. \n",
    "\n",
    "One important thing to know about BastionLab is that it supports models with an arbitrary number of inputs, but it only supports models with a single output. This is the first step we need to address as Hugging Face's models typically have several outputs (*logits, loss, etc*).\n",
    "\n",
    "We'll use BastionLab's utility wrapper to select only one output of the model. In our case: the one that corresponds with the logits. \n",
    "\n",
    "***# COMMENTS IN THE CODE =)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultipleOutputWrapper(nn.Module):\n",
    "    \"\"\"Utility wrapper to select one output of a model with multiple outputs.\n",
    "\n",
    "    Args:\n",
    "        module: A model with more than one output.\n",
    "        output: Index of the output to retain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module, output: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.inner = module\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
    "        output = self.inner.forward(*args, **kwargs)\n",
    "        return output[self.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(\n",
    "    model, 0\n",
    ")  # This can be loaded from bastionlab.torch.utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online phase - dataset and model upload and training\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that both dataset and model are prepared, we can upload them securely to the secure enclave for the training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data owner's side: uploading the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will connect to the BastionLab Torch instance using our `Connection` library. \n",
    "\n",
    "Once connected, we'll the `RemoteDataset()` method to upload the datasets inside the enclave. The method needs us to provide a name, and set a Differential Privacy budget. Here we put `1 000 000` an arbitrary number, but as a rule of thumb, it should be much lower, such as 4 or 8. \n",
    "\n",
    "***# Why don't we use the rule of thumb budget? and why should it be lower?***\n",
    "\n",
    "> *To learn more about Differential Privacy and why it's important you use it, you [can read this article](**HAVE A LINK TO AN INTERESTING ARTICLE ON THE TOPIC WHILE WE MAKE OURS ^^**).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending SMSSpamCollection: 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 8.86MB/s]\n",
      "Sending SMSSpamCollection (test): 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 20.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from bastionlab import Connection\n",
    "\n",
    "# The Data owner privately uploads their model online\n",
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_dataset = client.RemoteDataset(\n",
    "    train_set, validation_set, name=\"SMSSpamCollection\", privacy_limit=1_000_000.0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: uploading the model and trigger training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's finally time to train the model! \n",
    "\n",
    "The data scientist will use the `list_remote_datasets` endpoint to get a list of the available datasets on the server that they'll be able to use for training.\n",
    "\n",
    "***# comments in the code! what is str(ds) blabla***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMSSpamCollection (5cd871636638c4cde41c672d735e2ab0af1628edc43ccd4bcdbc126c35e95fa1): size=64, desc=N/A']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_datasets = client.list_remote_datasets()\n",
    "\n",
    "[str(ds) for ds in remote_datasets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset uploaded previously is available as a `RemoteDataset ` object. It is a pointer to the remote dataset uploaded previously, **that contains only metadata and nothing else**. This allows the data scientist to play with remote datasets without users' data being exposed in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bastionlab.torch.learner.RemoteDataset at 0x7f39bcb487c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send the model to the server and set all the necessary training parameters, we'll use the `RemoteLearner()` method. \n",
    "\n",
    "To start training, we'll 'call the `fit` method on the `RemoteLearner` object with an appropriate number of epochs and Differential Privacy budget. \n",
    "\n",
    "Then we'll test the model directly on the server with the `test()` method.\n",
    "\n",
    "***# what is appropriate? Are we sure epoch is clear to the reader?***\n",
    "\n",
    "> Note that behind the scenes, a DP-SGD training loop will be used. \n",
    ">\n",
    ">***# LINK TO PAPER? Why is that important and why are we mentionning this here?***\n",
    "\n",
    "Finally, and it's the last step of this tutorial, we'll retrieve a local copy of the trained model once the training is complete. To do so, we'll use the `get_model()` method.\n",
    "\n",
    "***# COMMENTS IN THE CODE. What are the given variables/arguments to the RemoteLearner method? what are the various arguments etc***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending DistilBERT: 100%|████████████████████| 268M/268M [00:05<00:00, 47.3MB/s] \n",
      "Epoch 1/2 - train:  12%|██▌                 | 4/32 [00:18<02:07,  4.54s/batch, cross_entropy=10.0000 (+/- 3656.3914)]  "
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "# The Data Scientist discovers available datasets and uses one of them to train their model\n",
    "client = Connection(\"localhost\").client.torch\n",
    "remote_learner = client.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")\n",
    "\n",
    "remote_learner.fit(nb_epochs=2, eps=6.0)\n",
    "remote_learner.test(metric=\"accuracy\")\n",
    "\n",
    "trained_model = remote_learner.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
