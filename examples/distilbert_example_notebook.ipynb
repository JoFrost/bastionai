{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bastion AI Real World Example\n",
    "## Finetuning DistilBERT for binary classification on the SMS Spam Collection\n",
    "\n",
    "Data preparation and training are largely based on https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894.\n",
    "\n",
    "### Installing Bastion AI\n",
    "\n",
    "### From source\n",
    "\n",
    "To use this notebook, you'll need a working Bastion AI installation.\n",
    "First clone our repo:\n",
    "```\n",
    "$ git clone git@github.com:mithril-security/bastionai.git\n",
    "```\n",
    "Then install the client library:\n",
    "```\n",
    "$ cd ./bastionai/client\n",
    "$ make install\n",
    "```\n",
    "\n",
    "### Via pip\n",
    "\n",
    "Just run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bastionai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing and importing additionnal packages\n",
    "\n",
    "Let's first import all the necessary packages for the entire notebook.\n",
    "The makefile for the client has already set up a virtualenv with the client dependences for us.\n",
    "We just need to install the additionnal packages we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers pandas sklearn ipykernel ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import necessary packages and objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bastionai.client import Connection, Private\n",
    "from bastionai.optimizer_config import Adam\n",
    "from bastionai.utils import MultipleOutputWrapper, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "The dataset can be found at https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip.\n",
    "Unzip the archive to obtain the datset file:\n",
    "\n",
    "```\n",
    "$ unzip smsspamcollection.zip\n",
    "```\n",
    "\n",
    "Each row represent a sample, the label come first followed by a tab and the raw text:\n",
    "```\n",
    "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "ham\tOk lar... Joking wif u oni...\n",
    "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "```\n",
    "\n",
    "We first load the data from the file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "  for line in f.readlines():\n",
    "    split = line.split('\\t')\n",
    "    labels.append(1 if split[0] == \"spam\" else 0)\n",
    "    texts.append(split[1])\n",
    "df = pd.DataFrame({ \"label\": labels, \"text\": texts })\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then preprocess the data using DistilBERT's tokenizer and we obtain tensors ready to be fed to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "for sample in df.text.values:\n",
    "    encoding_dict = tokenizer.encode_plus(\n",
    "        sample,\n",
    "        add_special_tokens=True,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    token_id.append(encoding_dict['input_ids']) \n",
    "    attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(df.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to split the data in a train and test sets and to wrap it inside Dataset and DataLoader objects for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(labels)),\n",
    "    test_size=val_ratio,\n",
    "    shuffle=True,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "train_set = TensorDataset([\n",
    "    token_id[train_idx], \n",
    "    attention_masks[train_idx]\n",
    "], labels[train_idx])\n",
    "\n",
    "test_set = TensorDataset([\n",
    "    token_id[test_idx], \n",
    "    attention_masks[test_idx]\n",
    "], labels[test_idx])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=4)\n",
    "test_dataloader = DataLoader(test_set, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the model for use with DP-SGD and Bastion AI\n",
    "\n",
    "We now turn to preparing the DistilBERT language model. As Hugging Face's models typically have several outputs (logits, loss, etc.) we use Bastion AI's utility wrapper for models with multiple outputs to select the sole output that corresponds with the logits. In fact, Bastion AI's server supports models with an arbitrtary number of inputs but only supports models with a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not display warnings about layer not initialized\n",
    "# with pretrained weights (classification layers, this is fine)\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True\n",
    ")\n",
    "model = MultipleOutputWrapper(model, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending dataset and model and training on the server\n",
    "\n",
    "Before proceeding, we need to start a local Bastion AI server which can be achivied with the following commands,\n",
    "assuming you have a working rust toolchain (https://www.rust-lang.org/tools/install):\n",
    "\n",
    "```\n",
    "$ cd ../server/bastionai_app\n",
    "$ cargo run\n",
    "```\n",
    "\n",
    "Now that the server code has been compiled and the server has started, it's time to send the dataset and the model to the server.\n",
    "\n",
    "We first use the `RemoteDataLoader` function to send our train and test dataloaders to the server (what we really send are the dataset and the dataloading parameters, not the dataloaders per se) and we provide a name and description to better identify them.\n",
    "\n",
    "Second, we use the `RemoteLearner` function to send the model to server and to set all the necessary training config.\n",
    "As training will be executed remotely, we need to script the model prior to sending it (i.e. compile it to Torch Script) which is automatically done in the `RemoteLearner` constructor. In case the model is not suited for scripting, which is generally the case with Hugging Face's models, the constructor automatically resorts to use tracing, which means the model is run locally on a small but representative input and the torch jit compiler tracks all functions that are called and compiles them on the fly. This approach, although more error prone (in certain cases the input may not activate some needed computation paths) is less picky that scripting and accepts nearly all models.\n",
    "\n",
    "In addition, as we'll use the DP-SGD algorithm for training, the constructor will also make the model compatible with Bastion AI's DP-SGD implementation. Unlike Opacus that uses backprop hooks to compute per-sample gradients, Bastion AI relies on normal autograd and modified layers that internally store expanded gradients (weight tensors have the same size in memory but are manipulated through expanded views that repeat them as many times as there are samples in a batch so that the gradient of these views are per-sample gradients). Per-samples gradient computation is key to DP-SGD and is one ingredient that make DP usable with Deep Learning models.\n",
    "\n",
    "To start training, we just call the `fit` method on the `RemoteLerner` object with appropriate number of epochs and DP budget. We can optionally override some of the learner's settings such as the learning rate or the clipping factor.\n",
    "\n",
    "We may finally retrieve a local copy of the trained model once the training is complete with the `get_model` method and test the model directly on the server with the `test` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "with Connection(\"localhost\", 50051) as client:\n",
    "    remote_dataloader = client.RemoteDataLoader(train_dataloader, name=\"SMSSpamCollection\", privacy_limit=Private(302.1))\n",
    "    \n",
    "    remote_learner = client.RemoteLearner(\n",
    "        model,\n",
    "        remote_dataloader,\n",
    "        metric=\"cross_entropy\",\n",
    "        optimizer=Adam(lr=5e-5),\n",
    "        model_name=\"DistilBERT\",\n",
    "    )\n",
    "\n",
    "    remote_learner.fit(nb_epochs=2, eps=Private(22.0), metric_eps=Private(140.0))\n",
    "    remote_learner.test(metric=\"accuracy\", metric_eps=Private(140.0))\n",
    "    \n",
    "    trained_model = remote_learner.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfb725626286d8c8fc5334ffe77697f720dc23e64d3046271825a5556b528e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
